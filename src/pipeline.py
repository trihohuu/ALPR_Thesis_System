import cv2
import os
import time
from core.detector import PlateDetector
from core.ocr_paddle import LicensePlateOCR
from core.image_utils import preprocess_plate, draw_results

class ALPRPipeline:
    def __init__(self, yolo_path, use_gpu=True):
        print("üöÄ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng ALPR...")
        
        # 1. Kh·ªüi t·∫°o Detector (YOLO)
        if not os.path.exists(yolo_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model YOLO t·∫°i: {yolo_path}")
        self.detector = PlateDetector(model_path=yolo_path)
        
        # 2. Kh·ªüi t·∫°o OCR (Paddle)
        self.ocr = LicensePlateOCR(use_gpu=use_gpu)
        
        print("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")

    def _process_frame(self, frame):
        """
        H√†m x·ª≠ l√Ω n·ªôi b·ªô cho 1 khung h√¨nh (D√πng chung cho c·∫£ ·∫¢nh v√† Video)
        Input: ·∫¢nh g·ªëc
        Output: ·∫¢nh ƒë√£ v·∫Ω box + th√¥ng tin bi·ªÉn s·ªë
        """
        # B1: Detect
        detections = self.detector.detect(frame)
        
        # B2: OCR t·ª´ng bi·ªÉn s·ªë
        final_results = []
        for item in detections:
            plate_img = item['plate_img']
            
            # Ti·ªÅn x·ª≠ l√Ω (Padding, Resize,...) - Nh∆∞ ƒë√£ fix ·ªü b∆∞·ªõc tr∆∞·ªõc
            processed_plate = preprocess_plate(plate_img)
            
            # OCR
            text, conf = self.ocr.predict(processed_plate)
            
            item['text'] = text
            item['ocr_conf'] = conf
            final_results.append(item)
            
            # Ch·ªâ in log n·∫øu ƒë·ªô tin c·∫≠y cao ƒë·ªÉ ƒë·ª° spam terminal khi ch·∫°y video
            if conf > 0.5:
                print(f"   -> Bi·ªÉn s·ªë: {text} (Conf: {conf:.2f})")

        # B3: V·∫Ω k·∫øt qu·∫£
        result_frame = draw_results(frame, final_results)
        return result_frame

    def run(self, source_path, show=True, save_path=None):
        """
        T·ª± ƒë·ªông nh·∫≠n di·ªán ·∫¢nh ho·∫∑c Video ƒë·ªÉ x·ª≠ l√Ω
        """
        if not os.path.exists(source_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {source_path}")
            return

        # Ki·ªÉm tra ƒëu√¥i file ƒë·ªÉ bi·∫øt l√† ·∫£nh hay video
        ext = os.path.splitext(source_path)[1].lower()
        video_exts = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']

        if ext in video_exts:
            self._process_video(source_path, show, save_path)
        else:
            self._process_image(source_path, show, save_path)

    def _process_image(self, img_path, show, save_path):
        print(f"\nüñºÔ∏è ƒêang x·ª≠ l√Ω ·∫£nh: {img_path}")
        frame = cv2.imread(img_path)
        if frame is None:
            print("L·ªói ƒë·ªçc ·∫£nh!")
            return

        start = time.time()
        processed_frame = self._process_frame(frame)
        print(f"‚è±Ô∏è Th·ªùi gian: {time.time() - start:.4f}s")

        if save_path:
            cv2.imwrite(save_path, processed_frame)
            print(f"üíæ ƒê√£ l∆∞u t·∫°i: {save_path}")

        if show:
            self._show_result(processed_frame, wait_duration=0) # 0 = ƒê·ª£i b·∫•m ph√≠m

    def _process_video(self, video_path, show, save_path):
        print(f"\nüé• ƒêang x·ª≠ l√Ω video: {video_path}")
        cap = cv2.VideoCapture(video_path)
        
        # L·∫•y th√¥ng s·ªë video
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        writer = None
        if save_path:
            # T·∫°o video writer (MP4)
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))

        frame_idx = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_idx += 1
            print(f"Frame {frame_idx}/{total_frames}...", end='\r') # In ƒë√® d√≤ng ƒë·ªÉ ƒë·ª° spam
            
            # X·ª≠ l√Ω frame
            processed_frame = self._process_frame(frame)
            
            # L∆∞u video
            if writer:
                writer.write(processed_frame)
            
            # Hi·ªÉn th·ªã
            if show:
                # waitKey(1) ƒë·ªÉ video t·ª± ch·∫°y, nh·∫•n 'q' ƒë·ªÉ tho√°t
                if self._show_result(processed_frame, wait_duration=1) == ord('q'):
                    print("\nƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng.")
                    break

        cap.release()
        if writer:
            writer.release()
        print(f"\n‚úÖ Ho√†n t·∫•t! Video ƒë√£ l∆∞u t·∫°i: {save_path}")

    def _show_result(self, image, wait_duration=0):
        # Resize n·∫øu ·∫£nh qu√° to ƒë·ªÉ hi·ªÉn th·ªã v·ª´a m√†n h√¨nh
        h, w = image.shape[:2]
        if w > 1200:
            scale = 1200 / w
            image = cv2.resize(image, (1200, int(h * scale)))
        
        cv2.imshow("ALPR System (Teddy)", image)
        return cv2.waitKey(wait_duration)

# ==========================================
# MAIN
# ==========================================
if __name__ == "__main__":
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    MODEL_PATH = os.path.join(CURRENT_DIR, '..', 'models', 'yolo', 'weights', 'best.pt')
    
    # --- TEST VIDEO HO·∫∂C ·∫¢NH T·∫†I ƒê√ÇY ---
    # B·∫°n thay ƒë·ªïi t√™n file ·ªü ƒë√¢y l√† ƒë∆∞·ª£c
    # INPUT_FILE = os.path.join(CURRENT_DIR, '..', 'assets', 'test_image5.jpg') 
    INPUT_FILE = os.path.join(CURRENT_DIR, '..', 'assets', 'ducky.mp4') # V√≠ d·ª• test video

    # T√™n file ƒë·∫ßu ra t·ª± ƒë·ªông
    filename = os.path.basename(INPUT_FILE)
    OUTPUT_FILE = os.path.join(CURRENT_DIR, '..', 'output', 'result_' + filename)

    try:
        # Nh·ªõ set use_gpu=False n·∫øu ƒëang ch·∫°y CPU ƒë·ªÉ tr√°nh l·ªói Segfault
        app = ALPRPipeline(yolo_path=MODEL_PATH, use_gpu=False)
        
        # Ch·∫°y pipeline
        app.run(INPUT_FILE, save_path=OUTPUT_FILE, show=False)
        
    except Exception as e:
        print(f"‚ùå L·ªói Fatal: {e}")